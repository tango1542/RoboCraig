
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Auto Generated Documentation &#8212; RoboCopy 1.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />

  <link rel="stylesheet" href="_static/custom.css" type="text/css" />


  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">


          <div class="body" role="main">

  <div class="section" id="auto-generated-documentation">
<h1>Code<a class="headerlink" href="#auto-generated-documentation" title="Permalink to this headline">Â¶</a></h1>
</div>
<h3>Database Models</h3>
<xmp>
    @login_manager.user_loader
    def load_user(user_id):
        return User.query.get(int(user_id))

    class User(db.Model, UserMixin):
        id = db.Column(db.Integer, primary_key=True)
        username = db.Column(db.String(20), unique=True, nullable=False)
        email = db.Column(db.String(120), unique=True, nullable=False)
        password = db.Column(db.String(60), nullable=False)
        searches = db.relationship('Searcher', backref = 'author', lazy=True)       #This is noting that there is a reference to the Searcher object

        def __repr__(self):
            return f"User('{self.username}', '{self.email}')"

    class Searcher (db.Model):
        id = db.Column(db.Integer, primary_key=True)
        category = db.Column(db.String(20), nullable=False)
        search_term = db.Column(db.String(50), nullable=False)
        zip_code = db.Column(db.String(10), nullable=False)  #Might need this to be int, or validate to 5 digits
        max_distance = db.Column(db.String(10), nullable=False)
        max_price = db.Column(db.String(10), nullable=False)
        user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)       #This is making a reference to the user_id in the users table

        def __repr__(self):
            return f"Search('{self.category}','{self.search_term}', '{self.zip_code}','{self.max_distance}','{self.max_price}')"

</xmp>

<h3>Forms</h3>
<xmp>
    class RegistrationForm(FlaskForm):
    username = StringField('Username',validators=[DataRequired(), Length(min=2, max=20)])
    email = StringField('Email',validators=[DataRequired(), Email()])
    password = PasswordField('Password', validators=[DataRequired()])
    confirm_password = PasswordField('Confirm Password',validators=[DataRequired(), EqualTo('password')])
    submit = SubmitField('Sign Up')

    def validate_username(self, username):      #Queries the user in the database.  If that user exists, it will not allow that username to be created
        user = User.query.filter_by(username=username.data).first()
        if user:
            raise ValidationError('That username is taken.  Please choose a different one')

    def validate_email(self, email):    #Queries the email in the database.  If that email exists, it will not allow that username to be created
        user = User.query.filter_by(email=email.data).first()
        if user:
            raise ValidationError('That email has already been registered.  Please choose another one')

    class LoginForm(FlaskForm):
        email = StringField('Email',validators=[DataRequired(), Email()])
        password = PasswordField('Password', validators=[DataRequired()])
        remember = BooleanField('Remember Me')
        submit = SubmitField('Login')

    class UpdateAccountForm(FlaskForm):     #not yet using this form
        username = StringField('Username',validators=[DataRequired(), Length(min=2, max=20)])
        email = StringField('Email',validators=[DataRequired(), Email()])
        picture = FileField('Update Profile Picture', validators=[FileAllowed(['jpg', 'png'])])
        submit = SubmitField('Update')

        def validate_username(self, username):
            if username.data != current_user.username:
                user = User.query.filter_by(username=username.data).first()
                if user:
                    raise ValidationError('That username is taken. Please choose a different one.')

        def validate_email(self, email):
            if email.data != current_user.email:
                user = User.query.filter_by(email=email.data).first()
                if user:
                    raise ValidationError('That email is taken. Please choose a different one.')



    class SearchForm(FlaskForm):        #using the choices to give the user a dropdown of options that will be used in the Craigslist URL
        category = SelectField('Category', choices=[('ata','Antiques'),('ppa','Appliances'),('ara','Arts & Crafts'),('sna','ATV/UTV/SNO'),('pta','AutoParts'),
                                                    ('ava','Aviation'),('baa','Baby and Kid Stuff'),('haa','Beauty and Health'),('bip','Bike Parts'),
                                                    ('bia','Bikes'),('bpa','Boat Parts'),('boo','Boats'),('bka','Books') ,('bfa','Business') ,
                                                    ('cta','Cars & Trucks') ,('ema','CDs/DVDs/VHSs') ,('moa','Cell Phones') ,('cla','Clothes & Acc') ,
                                                    ('cba','Collectibles') ,('syp','Computer Parts') ,('sya','Computers') ,('ela','Electronics'),
                                                    ('gra','Farm & Garden') ,('fua','Furniture') ,('foa','General') ,('hva','Heavy Equipment') ,('hsa','Household'),
                                                    ('jwa','Jewelry') ,('maa','Materials') ,('mpa','Motorcycle Parts') ,('mca','Motorcycles') ,
                                                    ('msa','Music Instruments') ,('pha','Photo & Video') ,('rva','RVs & Camp') ,('sga','Sporting'),
                                                    ('tia','Tickets') ,('tla','Tools') ,('taa','Toys & Games') ,('tra','Trailers') ,
                                                    ('vga','Video Gaming') ,('wta','Wheels & Tires')], validators=[DataRequired()])

        search_term = StringField('Search Term', validators=[DataRequired()])
        zip_code = StringField('Zip Code', validators=[DataRequired()])
        max_distance = StringField('Max Distance', validators=[DataRequired()])
        max_price = StringField('Max Price', validators=[DataRequired()])
        submit = SubmitField('Save Search')

</xmp>

<h3>Scraped Results</h3>
<xmp>
    @app.route("/search_results/<string:username>")     #This route takes constructs the URL(s), and parses them using Beautiful Soup
    def scrapedresults(username):

        user = User.query.filter_by(username=username).first_or_404()       #This gets the current user from the db
        searches = Searcher.query.filter_by(author=user)        #This gets all of the current users saved searchs from the db

        search_term_rep_list = []       #Can maybe delete

        catz=[]
        constructed_urls = []       #This will be the list where completed URLs are contained
        for search in searches:
            print("This is search: " + str(search))
            search_term_replace = search.search_term.replace(" ","+")  #Putting this here so the search term is concatenated with +'s in the craigslist request url
            print ("This is search_term_replace: " + str(search_term_replace))
            search_term_rep_list.append(search_term_replace)    #Putting the formatted search terms into the list
            print ("This is search.category: " + str(search.category))

            #This is the URL request to Craigslist which contain variable names
            baseurl = 'https://minneapolis.craigslist.org/search/' + search.category + '?query=' + search_term_replace + '&srchType=T&hasPic=1&search_distance=' + search.max_distance + '&postal=' + search.zip_code + '&min_price=3&max_price=' + search.max_price
            constructed_urls.append(baseurl)        #Putting the completed urls into the contructed_urls list
            print("This is baseurl: " + baseurl)

            print("search.search_term" + str(search.search_term) + " search.zip_code: " + str(search.zip_code) + " search.max_distance: " + str(search.max_distance) + " search.max_price: " + str(search.max_price))

        print("This is constructed_urls list: ")
        for url in constructed_urls:
            print("url: " + url)

        print ("This is constructed_urls")
        print (constructed_urls)

        #Making lists of the five items that need to be zipped together to create a list of lists
        titles_list = []
        prices_list_temp = []  # needed to make a temp list because bs was finding two prices on the page
        date_posted_list = []
        images_list = []
        new_url_list = []
        term_list = []     #This list will be able to pass the name of the search to the saved search

        for url in constructed_urls:        #for each url in the saved_searches, Beautiful Soup will take it and scrape the HTML
            print ("This is url")
            print (url)
            html_page = urllib.request.urlopen(url)
            soup = BeautifulSoup(html_page, "lxml")
            term = soup.find("input",{'name':'query'})['value']     #Getting the input box from BS4 to be able to get the name of the search
            print ("This is term: " + term)

            for post in soup.find_all('li', "result-row"):
                term_list.append(term)

                # query.flatinput.ui - autocomplete - input.ui - autocomplete - loading

                for po in post.find_all("a","result-title hdrlnk"):  #titles loop, works properly
                    titles_list.append(po.text)

                for price in post.find_all('span', 'result-price'):      #price loop
                    prices_list_temp.append(price.text)

                for dat in post.find_all("time","result-date"):      #date posted loop
                    date_posted_list.append(dat.text)

                for images in post.find_all("a","result-image gallery"):      #image id loop
                    pic_id=images['data-ids']
                    cut_id = pic_id[2:19]  # This splits the returned data into just the id of the photo to be displayed
                    full_pic_url = "https://images.craigslist.org/" + cut_id + "_300x300.jpg"   #This is the full url for the image
                    images_list.append(full_pic_url)

                for url in post.find_all("a","result-title hdrlnk"):        #anchor link
                    url_link = (url["href"])
                    new_url_list.append(url_link)

        print ("This is term_list")
        print (term_list)
        prices_list = [i for a, i in enumerate(prices_list_temp) if a % 2 == 0]  # using this to get every other price because it was duplicating by finding two prices when scraping

        #This is the list of lists being zipped together to create saved_searches
        zipped_searches = list(zip(titles_list,prices_list,date_posted_list,images_list,new_url_list,term_list))

        for item in zipped_searches:
            print (item)

        if current_user == user:
            return render_template('search_results.html',zipped_searches=zipped_searches,username=username)     #Directed to search_results page, passing on the zipped_searches
        else:
            abort (403)


</xmp>
          </div>

        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h3 class="logo"><a href="index.html">RoboCopy</a></h3>
<h3 class="logo"><a href="project.html">Project</a></h3>
<h3 class="logo"><a href="code.html">Code</a></h3>
<br>


<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, Jeff Peterson.

      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>

      |
      <a href="_sources/code.rst.txt"
          rel="nofollow">Page source</a>
    </div>




  </body>
</html>
